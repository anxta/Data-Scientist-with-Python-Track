{"cells":[{"source":"## Chapter 1","metadata":{},"cell_type":"markdown","id":"db77c5c2-0d9e-4bd5-abb5-3141ae5edcdd"},{"source":"### Train your first classification tree","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":" # Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=6, random_state=SEED)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\nprint(y_pred[0:5])","metadata":{},"id":"7f26dd6a-20aa-4d19-8d3f-b8f64fba1821","cell_type":"code","execution_count":1,"outputs":[]},{"source":"### Evaluate the classification tree","metadata":{},"cell_type":"markdown","id":"3530bf35-b53a-47bf-9507-d9d0b93af5ad"},{"source":"# Import accuracy_score\nfrom sklearn.metrics import accuracy_score\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\n\n# Compute test set accuracy  \nacc = accuracy_score(y_test, y_pred)\nprint(\"Test set accuracy: {:.2f}\".format(acc))","metadata":{},"cell_type":"code","id":"3644f9eb-ccb6-4320-9e53-6e1640db4f02","execution_count":null,"outputs":[]},{"source":"### Logistic Regression vs Classification tree","metadata":{},"cell_type":"markdown","id":"b777b1d3-8b9a-43ba-8ce4-be83fe3eef5b"},{"source":"# Import LogisticRegression from sklearn.linear_model\nfrom sklearn.linear_model import  LogisticRegression\n\n# Instatiate logreg\nlogreg = LogisticRegression(random_state=1)\n\n# Fit logreg to the training set\nlogreg.fit(X_train, y_train)\n\n# Define a list called clfs containing the two classifiers logreg and dt\nclfs = [logreg, dt]\n\n# Review the decision regions of the two classifiers\nplot_labeled_decision_regions(X_test, y_test, clfs)","metadata":{},"cell_type":"code","id":"2f6211b0-9acb-4102-b3e9-989cca1fd8af","execution_count":null,"outputs":[]},{"source":"### Using entropy as a criterion","metadata":{},"cell_type":"markdown","id":"4e39d4ac-cadc-4c48-83a9-faa7c16ffb18"},{"source":"# Import DecisionTreeClassifier from sklearn.tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Instantiate dt_entropy, set 'entropy' as the information criterion\ndt_entropy = DecisionTreeClassifier(max_depth=8, criterion='entropy', random_state=1)\n\n# Fit dt_entropy to the training set\ndt_entropy.fit(X_train, y_train)","metadata":{},"cell_type":"code","id":"71195e79-9a8f-4981-a2a8-9f5de07d61d4","execution_count":null,"outputs":[]},{"source":"### Entropy vs Gini index","metadata":{},"cell_type":"markdown","id":"b5240cfc-a6db-41b4-a91e-9d9354f69a5c"},{"source":"# Import accuracy_score from sklearn.metrics\nfrom sklearn.metrics import accuracy_score\n\n# Use dt_entropy to predict test set labels\ny_pred= dt_entropy.predict(X_test)\n\n# Evaluate accuracy_entropy\naccuracy_entropy = accuracy_score(y_test, y_pred)\n\n# Print accuracy_entropy\nprint(f'Accuracy achieved by using entropy: {accuracy_entropy:.3f}')\n\n# Print accuracy_gini\nprint(f'Accuracy achieved by using the gini index: {accuracy_gini:.3f}')","metadata":{},"cell_type":"code","id":"7ed7ede6-9991-4a6d-8dc0-342dd15c515e","execution_count":null,"outputs":[]},{"source":"### Train your first regression tree","metadata":{},"cell_type":"markdown","id":"0e4c8a3b-580a-4fa1-af7c-8b70e3ea4b26"},{"source":"# Import DecisionTreeRegressor from sklearn.tree\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Instantiate dt\ndt = DecisionTreeRegressor(max_depth=8,\n             min_samples_leaf=0.13,\n            random_state=3)\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)","metadata":{},"cell_type":"code","id":"2fc8190f-0076-47eb-8947-8502693fd7bd","execution_count":null,"outputs":[]},{"source":"### Evaluate the regression tree","metadata":{},"cell_type":"markdown","id":"e34615e4-74f7-4d86-a22d-2460154ca866"},{"source":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute y_pred\ny_pred = dt.predict(X_test)\n\n# Compute mse_dt\nmse_dt = MSE(y_test, y_pred)\n\n# Compute rmse_dt\nrmse_dt = mse_dt**(1/2) \n\n# Print rmse_dt\nprint(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))","metadata":{},"cell_type":"code","id":"6326ea45-469c-45b9-9c8d-1dfa3e006c40","execution_count":null,"outputs":[]},{"source":"### Linear regression vs regression tree","metadata":{},"cell_type":"markdown","id":"7cf3dc6b-e314-41ba-9a81-e5376a222ff7"},{"source":"# Predict test set labels \ny_pred_lr = lr.predict(X_test)\n\n# Compute mse_lr\nmse_lr = MSE(y_test, y_pred_lr)\n\n# Compute rmse_lr\nrmse_lr = mse_lr**(1/2)\n\n# Print rmse_lr\nprint('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n\n# Print rmse_dt\nprint('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))","metadata":{},"cell_type":"code","id":"98f9a246-e4e6-4325-b955-b6112c0acee3","execution_count":null,"outputs":[]},{"source":"## Chapter 2","metadata":{},"cell_type":"markdown","id":"5af8ef32-d463-4129-aae3-eff12e81db71"},{"source":"### Instantiate the model","metadata":{},"cell_type":"markdown","id":"e3f6ed56-26a8-4599-ab5e-e7cb82a3074f"},{"source":"# Import train_test_split from sklearn.model_selection\nfrom sklearn.model_selection import train_test_split\n\n# Set SEED for reproducibility\nSEED = 1\n\n# Split the data into 70% train and 30% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)\n\n# Instantiate a DecisionTreeRegressor dt\ndt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=SEED)","metadata":{},"cell_type":"code","id":"f8d63958-34b6-4f30-9689-8d8993b90236","execution_count":null,"outputs":[]},{"source":"### Evaluate the 10-fold CV error","metadata":{},"cell_type":"markdown","id":"b66252c2-bef6-44cc-bd2e-a8ad867f834f"},{"source":"# Compute the array containing the 10-folds CV MSEs\nMSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n                       scoring='neg_mean_squared_error',\n                       n_jobs=-1)\n\n# Compute the 10-folds CV RMSE\nRMSE_CV = (MSE_CV_scores.mean())**(1/2)\n\n# Print RMSE_CV\nprint('CV RMSE: {:.2f}'.format(RMSE_CV))","metadata":{},"cell_type":"code","id":"d7c58886-f74a-4f54-a279-3934906c5b1a","execution_count":null,"outputs":[]},{"source":"### Evaluate the training error","metadata":{},"cell_type":"markdown","id":"9e6b333f-df9c-4aaa-a001-64cc76324f3b"},{"source":"# Import mean_squared_error from sklearn.metrics as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit dt to the training set\ndt.fit(X_train, y_train)\n\n# Predict the labels of the training set\ny_pred_train = dt.predict(X_train)\n\n# Evaluate the training set RMSE of dt\nRMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n\n# Print RMSE_train\nprint('Train RMSE: {:.2f}'.format(RMSE_train))","metadata":{},"cell_type":"code","id":"8d93ba66-81c8-4c6e-888c-3ca591f390de","execution_count":null,"outputs":[]},{"source":"### Define the ensemble","metadata":{},"cell_type":"markdown","id":"acbbfdbb-cb00-49ef-a3ed-b9e695ead2ac"},{"source":"# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED)\n\n# Instantiate knn\nknn = KNN(n_neighbors=27)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=0.13, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]","metadata":{},"cell_type":"code","id":"0d301154-dd2d-4a73-90c7-c094845f546b","execution_count":null,"outputs":[]},{"source":"### Evaluate individual classifiers","metadata":{},"cell_type":"markdown","id":"f328509d-f4e0-411b-a2bf-1c8eaf763187"},{"source":"# Iterate over the pre-defined list of classifiers\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","metadata":{},"cell_type":"code","id":"8cf44335-37c7-4e7d-86cb-694dd5019ead","execution_count":null,"outputs":[]},{"source":"### Better performance with a Voting Classifier","metadata":{},"cell_type":"markdown","id":"6cfc514a-090d-43d5-8185-20baef8b0ea7"},{"source":"# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train, y_train)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_test, y_pred)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","metadata":{},"cell_type":"code","id":"f52e2f3d-bb16-421c-bb0f-d2d46305b20c","execution_count":null,"outputs":[]},{"source":"## Chapter 3","metadata":{},"cell_type":"markdown","id":"cf10c18a-570b-4aa0-be31-9e7c2c8fdf01"},{"source":"### Define the bagging classifier","metadata":{},"cell_type":"markdown","id":"a72ca0b8-127c-491c-abd9-a01fb435ad3b"},{"source":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)","metadata":{},"cell_type":"code","id":"bcd3bd9e-38c3-4e0d-b029-f0536b585f9c","execution_count":null,"outputs":[]},{"source":"### Evaluate Bagging performance","metadata":{},"cell_type":"markdown","id":"404dc814-1a30-4add-8a99-8980b2e9f3f6"},{"source":"# Fit bc to the training set\nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_test)\n\n# Evaluate acc_test\nacc_test = accuracy_score(y_test, y_pred)\nprint('Test set accuracy of bc: {:.2f}'.format(acc_test)) ","metadata":{},"cell_type":"code","id":"0317782b-93f1-40ca-80da-34ba4d1d5bd1","execution_count":null,"outputs":[]},{"source":"### Prepare the ground","metadata":{},"cell_type":"markdown","id":"c2b20eca-32f4-4547-83c4-f67a4461834e"},{"source":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)\n\n# Instantiate bc\nbc = BaggingClassifier(base_estimator=dt, \n            n_estimators=50,\n            oob_score=True,\n            random_state=1)","metadata":{},"cell_type":"code","id":"9b3712c7-6fdb-4ede-b813-01d3747585fd","execution_count":null,"outputs":[]},{"source":"### OOB Score vs Test Set Score","metadata":{},"cell_type":"markdown","id":"2e30f53f-df44-49e0-8327-d24b5e31b0ec"},{"source":"# Fit bc to the training set \nbc.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = bc.predict(X_test)\n\n# Evaluate test set accuracy\nacc_test = accuracy_score(y_test, y_pred)\n\n# Evaluate OOB accuracy\nacc_oob = bc.oob_score_\n\n# Print acc_test and acc_oob\nprint('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))","metadata":{},"cell_type":"code","id":"82ee6582-6476-42eb-bcc4-ce7cba3bca06","execution_count":null,"outputs":[]},{"source":"### Train an RF regressor","metadata":{},"cell_type":"markdown","id":"2af58fbd-8fdd-4447-bd0f-5f0ea3f9a166"},{"source":"# Import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate rf\nrf = RandomForestRegressor(n_estimators=25,\n            random_state=2)\n            \n# Fit rf to the training set    \nrf.fit(X_train, y_train) ","metadata":{},"cell_type":"code","id":"8566155c-b6d4-49da-92ad-eedaa03f330c","execution_count":null,"outputs":[]},{"source":"### Evaluate the RF regressor","metadata":{},"cell_type":"markdown","id":"402887b1-caa5-4b87-ad4e-3c29e1c1e047"},{"source":"# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Predict the test set labels\ny_pred = rf.predict(X_test)\n\n# Evaluate the test set RMSE\nrmse_test = MSE(y_test, y_pred)**(1/2)\n\n# Print rmse_test\nprint('Test set RMSE of rf: {:.2f}'.format(rmse_test))","metadata":{},"cell_type":"code","id":"dc817299-b35e-44f4-9d3d-53a7f50ca4dd","execution_count":null,"outputs":[]},{"source":"### Visualizing features importances","metadata":{},"cell_type":"markdown","id":"63b61048-21d5-49c0-a1e0-18acb66e3a42"},{"source":"# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()","metadata":{},"cell_type":"code","id":"5846b14a-c581-4d8c-b876-c6237e4cf4ad","execution_count":null,"outputs":[]},{"source":"## Chapter 4","metadata":{},"cell_type":"markdown","id":"99221d3b-4af8-4523-b50e-0908dc9e0abb"},{"source":"### Define the AdaBoost classifier","metadata":{},"cell_type":"markdown","id":"c911afac-87d5-40b2-8dce-7b0eef2ba6ee"},{"source":"# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)","metadata":{},"cell_type":"code","id":"2d61211a-dcad-4fe5-8c2a-8255325265fc","execution_count":null,"outputs":[]},{"source":"### Train the AdaBoost classifier","metadata":{},"cell_type":"markdown","id":"05856981-1b67-469e-8804-5bf7fee8fcb9"},{"source":"# Fit ada to the training set\nada.fit(X_train, y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_proba = ada.predict_proba(X_test)[:, 1]\n","metadata":{},"cell_type":"code","id":"bdb91531-8323-42ab-b3ee-3a845b64ec96","execution_count":null,"outputs":[]},{"source":"### Evaluate the AdaBoost classifier","metadata":{},"cell_type":"markdown","id":"b9d0cd75-53db-41aa-a367-5d88679b5e55"},{"source":"# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Evaluate test-set roc_auc_score\nada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_roc_auc))","metadata":{},"cell_type":"code","id":"c42044fd-a267-48f0-b054-8e6413c997a8","execution_count":null,"outputs":[]},{"source":"### Define the GB regressor","metadata":{},"cell_type":"markdown","id":"1cdc0f24-8709-4283-8e06-11775e4cad25"},{"source":"# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate gb\ngb = GradientBoostingRegressor(max_depth=4, \n            n_estimators=200,\n            random_state=2)","metadata":{},"cell_type":"code","id":"ee38010a-d447-4e5a-a1f1-cf833579e1c5","execution_count":null,"outputs":[]},{"source":"### Train the GB regressor","metadata":{},"cell_type":"markdown","id":"9c875004-de52-4806-aa5c-18f66121ec0b"},{"source":"# Fit gb to the training set\ngb.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = gb.predict(X_test)","metadata":{},"cell_type":"code","id":"43b9487d-c78a-421d-a4b4-dcabbdfd3776","execution_count":null,"outputs":[]},{"source":"### Evaluate the GB regressor","metadata":{},"cell_type":"markdown","id":"7828cde7-ac3e-4df8-aa3d-e9b51af82746"},{"source":"# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute MSE\nmse_test = MSE(y_test, y_pred)\n\n# Compute RMSE\nrmse_test = (mse_test.mean())**(1/2)\n\n# Print RMSE\nprint('Test set RMSE of gb: {:.3f}'.format(rmse_test))","metadata":{},"cell_type":"code","id":"8588eab9-e3dd-49ac-ad07-683764aede20","execution_count":null,"outputs":[]},{"source":"### Regression with SGB","metadata":{},"cell_type":"markdown","id":"ddfd4ffa-caf0-445e-a89c-3b0d36ce5c35"},{"source":"# Import GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Instantiate sgbr\nsgbr = GradientBoostingRegressor(max_depth=4, \n            subsample=0.9,\n            max_features=0.75,\n            n_estimators=200,\n            random_state=2)","metadata":{},"cell_type":"code","id":"67d63d0b-809d-4650-8da3-640e7ff27729","execution_count":null,"outputs":[]},{"source":"### Train the SGB regressor","metadata":{},"cell_type":"markdown","id":"42ada81a-a17a-4564-bfbb-6758308f3ae5"},{"source":"# Fit sgbr to the training set\nsgbr.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = sgbr.predict(X_test)","metadata":{},"cell_type":"code","id":"1bf6d001-d8e4-494a-9988-66ce97bd8acd","execution_count":null,"outputs":[]},{"source":"### Evaluate the SGB regressor","metadata":{},"cell_type":"markdown","id":"efbcf5f0-21d0-4d75-98e5-0f8c5bd21086"},{"source":"# Import mean_squared_error as MSE\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Compute test set MSE\nmse_test = MSE(y_test, y_pred)\n\n# Compute test set RMSE\nrmse_test = (mse_test.mean())**(1/2)\n\n# Print rmse_test\nprint('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))","metadata":{},"cell_type":"code","id":"bba03e36-db86-408c-a29c-aa02d9946bef","execution_count":null,"outputs":[]},{"source":"## Chapter 5","metadata":{},"cell_type":"markdown","id":"414b050d-685f-4c05-add6-c98262cf5ffa"},{"source":"### Set the tree's hyperparameter grid","metadata":{},"cell_type":"markdown","id":"7853097d-ee67-464f-9546-b4d8d7e4d883"},{"source":"# Define params_dt\nparams_dt = {\n        'max_depth' : [2, 3, 4],\n        'min_samples_leaf' : [0.12, 0.14, 0.16, 0.18]\n\n}","metadata":{},"cell_type":"code","id":"02efb9fe-b213-40a2-8b5d-428cfa560c8b","execution_count":null,"outputs":[]},{"source":"### Search for the optimal tree","metadata":{},"cell_type":"markdown","id":"3dd5f5e4-c785-45b0-be26-039c81cbafa2"},{"source":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Instantiate grid_dt\ngrid_dt = GridSearchCV(estimator=dt,\n                       param_grid=params_dt,\n                       scoring='roc_auc',\n                       cv=5,\n                       n_jobs=-1)","metadata":{},"cell_type":"code","id":"8fc92d59-6a76-4622-9125-a51491f5a136","execution_count":null,"outputs":[]},{"source":"# Import roc_auc_score from sklearn.metrics \nfrom sklearn.metrics import roc_auc_score\n\n# Extract the best estimator\nbest_model = grid_dt.best_estimator_\n\n# Predict the test set probabilities of the positive class\ny_pred_proba = best_model.predict_proba(X_test)[:,1]\n\n# Compute test_roc_auc\ntest_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print test_roc_auc\nprint('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))","metadata":{},"cell_type":"code","id":"2f3a8266-5049-491f-9de0-f0da9abb5bd5","execution_count":null,"outputs":[]},{"source":"### Set the hyperparameter grid of RF","metadata":{},"cell_type":"markdown","id":"5cce7696-9ebd-4bba-93a4-e1cfdef3665b"},{"source":"# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators' : [100, 350, 500],\n    'max_features' : ['log2', 'auto', 'sqrt'],\n    'min_samples_leaf' :[2, 10, 30]\n}","metadata":{},"cell_type":"code","id":"52b09849-8368-46b7-b09c-5347e126f59c","execution_count":null,"outputs":[]},{"source":"### Search for the optimal forest","metadata":{},"cell_type":"markdown","id":"3602240d-bbb3-4358-9ff6-e00d68a7c283"},{"source":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Instantiate grid_rf\ngrid_rf = GridSearchCV(estimator=rf,\n                       param_grid=params_rf,\n                       scoring='neg_mean_squared_error',\n                       cv=3,\n                       verbose=1,\n                       n_jobs=-1)","metadata":{},"cell_type":"code","id":"c8f29821-08ce-42e1-b7da-0665d19cb6dd","execution_count":null,"outputs":[]},{"source":"### Evaluate the optimal forest","metadata":{},"cell_type":"markdown","id":"3aaccf9d-fb61-4e2b-94f6-b0ffe7ba5a30"},{"source":"# Import mean_squared_error from sklearn.metrics as MSE \nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Extract the best estimator\nbest_model = grid_rf.best_estimator_\n\n# Predict test set labels\ny_pred = best_model.predict(X_test)\n\n# Compute rmse_test\nrmse_test =MSE(y_test, y_pred)**(1/2)\n\n# Print rmse_test\nprint('Test RMSE of best model: {:.3f}'.format(rmse_test)) ","metadata":{},"cell_type":"code","id":"207039e4-0da0-44fa-90e4-d35ae6e4e9c5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}